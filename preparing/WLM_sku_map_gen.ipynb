{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import os\n",
    "from flask import current_app\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_amz = pd.read_excel('data/mapping source/Amazon.xlsx', sheet_name='Master Sku Translation', dtype=str)\n",
    "\n",
    "amz_sku_mapping = source_amz[['seller-sku', 'SKU (FINAL)', 'Product','Product NZ']].copy()\n",
    "amz_sku_mapping = amz_sku_mapping.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "# rename columns to ['seller-sku', 'BS_SKU', 'B_SKU','B_NZ_SKU']\n",
    "amz_sku_mapping.columns = ['seller_sku', 'BS_SKU', 'B_SKU','B_NZ_SKU']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(directory):\n",
    "    data = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            name, _ = os.path.splitext(filename)\n",
    "            data[name] = pd.read_csv(os.path.join(directory, filename))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing/data/walmart/CA.csv\n",
    "\n",
    "dfs = read_files('data/Walmart/')\n",
    "\n",
    "# def combine_all_listings_by_SKU_Walmart(data_dict):\n",
    "#     \"\"\"\n",
    "#     This function combines Walmart product listings from multiple countries into a single DataFrame.\n",
    "#     Each country's DataFrame is processed to retain only the 'Supplier Part#' and 'Supplier ID' columns.\n",
    "#     The 'Supplier ID' column is renamed to indicate the country of origin.\n",
    "#     The processed DataFrames are then merged on the 'Supplier Part#' column.\n",
    "\n",
    "#     Parameters:\n",
    "#     data_dict (dict): A dictionary where keys are country names and values are DataFrames containing Walmart product listings.\n",
    "\n",
    "#     Returns:\n",
    "#     result_df (pandas.DataFrame): A DataFrame containing the combined product listings. Each row represents a unique product listing.\n",
    "#     The DataFrame contains columns for 'Supplier Part#' and 'Supplier ID_{country}' for each country.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def process_country(df, country):\n",
    "#         df = df[['Supplier Part#', 'Supplier ID']].copy()\n",
    "#         df.columns = ['seller_sku', f'Supplier_ID_{country}']\n",
    "#         return df\n",
    "\n",
    "#     processed_dfs = []\n",
    "#     for country, df in data_dict.items():\n",
    "#         country_code = country.split('_')[0].upper()  # Assuming filename format is like \"US_...\"\n",
    "#         processed_dfs.append(process_country(df, country_code))\n",
    "\n",
    "#     result_df = processed_dfs[0]\n",
    "#     for df in processed_dfs[1:]:\n",
    "#         result_df = pd.merge(result_df, df, on='seller_sku', how='outer')\n",
    "\n",
    "#     return result_df\n",
    "\n",
    "\n",
    "# combined_df = combine_all_listings_by_SKU_Walmart(dfs)\n",
    "\n",
    "\n",
    "CA_df = dfs['CA']\n",
    "US_df = dfs['US']\n",
    "\n",
    "US_df = US_df[['SKU', 'Item ID']]\n",
    "US_df.columns = ['seller_sku', 'item_id_US']\n",
    "\n",
    "CA_df = CA_df[['Seller Sku ID', 'Walmart SKU ID']]\n",
    "CA_df.columns = ['seller_sku', 'item_id_CA']\n",
    "\n",
    "combined_df = pd.merge(US_df, CA_df, on='seller_sku', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "final_NA_mapping = pd.merge(combined_df, amz_sku_mapping, on='seller_sku', how='left')\n",
    "\n",
    "final_NA_mapping.to_csv('results/Walmart_sku_mapping.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_sheet_in_chunks(worksheet, df, chunk_size=2000, continue_from_end=False, start_row=0):\n",
    "    try:\n",
    "        logger.info(f'Updating google sheet \"{worksheet }\" in chunks')\n",
    "        header = worksheet.row_values(1)\n",
    "        df_coloumns_list = df.columns.values.tolist()\n",
    "\n",
    "        if not continue_from_end:\n",
    "            worksheet.clear()\n",
    "            start_row = 0\n",
    "        else:\n",
    "            if header != df_coloumns_list:\n",
    "                logger.error(f'updating google sheet \"{worksheet}\" Header mismatch: {header} != {df_coloumns_list}')\n",
    "                raise Exception(f'updating google sheet \"{worksheet}\" Header mismatch: {header} != {df_coloumns_list}')\n",
    "                \n",
    "        worksheet_length = worksheet.row_count\n",
    "        df_columns_length = len(df.columns)\n",
    "\n",
    "        ensure_sheet_size(worksheet, worksheet_length + len(df),df_columns_length )\n",
    "\n",
    "        df_list = df_to_sheet_list(df, header= not continue_from_end)\n",
    "         \n",
    "        for i in range(start_row, len(df_list) + start_row, chunk_size):\n",
    "            chunk = df_list[i:i+chunk_size]\n",
    "            range_name = f'A{i+1}:{column_number_to_letter(df_columns_length)}{i+len(chunk)}'\n",
    "            logger.info(f'Updating chunk {range_name}')\n",
    "\n",
    "            worksheet.update(chunk, range_name)\n",
    "            # add timout to avoid google api rate limit\n",
    "            time.sleep(0.3)\n",
    "    except Exception as e:\n",
    "        logger.error(f'Error updating sheet in chunks: {str(e)}')\n",
    "\n",
    "\n",
    "def ensure_sheet_size(worksheet, required_rows, required_cols):\n",
    "    current_rows = worksheet.row_count\n",
    "    current_cols = worksheet.col_count\n",
    "\n",
    "    if required_rows > current_rows or required_cols > current_cols:\n",
    "        new_rows = max(required_rows, current_rows)\n",
    "        new_cols = max(required_cols, current_cols)\n",
    "        worksheet.resize(new_rows, new_cols)\n",
    "\n",
    "def df_to_sheet_list(df, header=True):\n",
    "    df =df.fillna('')\n",
    "    df = df.replace({pd.NA: ''})\n",
    "    df = df.replace({'nan': ''})\n",
    "    if header:\n",
    "        df = [df.columns.values.tolist()] + df.values.tolist()\n",
    "    else:\n",
    "        df = df.values.tolist()\n",
    "    return df\n",
    "\n",
    "def column_number_to_letter(n):\n",
    "    string = \"\"\n",
    "    while n > 0:\n",
    "        n, remainder = divmod(n - 1, 26)\n",
    "        string = chr(65 + remainder) + string\n",
    "    return string\n",
    "\n",
    "\n",
    "def get_workbook(sheet_id = \"1ZMzIMn7CzV_tUJSfXguHYLh3fkkgHVh_0u2NBWCzEAQ\"):\n",
    "    scopes = [\"https://www.googleapis.com/auth/spreadsheets\"]\n",
    "    creds = get_sheets_api_credentials()\n",
    "    client = gspread.authorize(creds)\n",
    "    workbook = client.open_by_key(sheet_id)\n",
    "    return workbook\n",
    "\n",
    "\n",
    "def get_sheets_api_credentials():\n",
    "    # Load environment variables\n",
    "    load_dotenv(override=True)\n",
    "    logger.info(\"Getting google sheets api credentials\")\n",
    "    # Create a dictionary with the credentials\n",
    "    cred_dict = {\n",
    "        \"type\": \"service_account\",\n",
    "        \"project_id\": \"w4l-inventory-update\",\n",
    "        \"private_key_id\": os.getenv(\"GOOGLE_PRIVATE_KEY_ID\"),\n",
    "        \"private_key\": os.getenv(\"GOOGLE_PRIVATE_KEY\"),   \n",
    "        \"client_email\": os.getenv(\"GOOGLE_CLIENT_EMAIL\"),\n",
    "        \"client_id\": os.getenv(\"GOOGLE_CLIENT_ID\"),\n",
    "        \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "        \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "        \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "        \"client_x509_cert_url\": os.getenv(\"GOOGLE_CLIENT_X509_CERT_URL\"),\n",
    "        \"universe_domain\": \"googleapis.com\"\n",
    "    }\n",
    "\n",
    "    # Create a temporary file to store the credentials\n",
    "    temp_cred_file = 'temp_credentials.json'\n",
    "    # if temp_cred_file is exist delate it first \n",
    "    if os.path.exists(temp_cred_file):\n",
    "        os.remove(temp_cred_file)\n",
    "        logger.info(\"Deleted the existing temp_cred_file\")\n",
    "\n",
    "    with open(temp_cred_file, 'w') as f:\n",
    "        json.dump(cred_dict, f)\n",
    "        logger.info(\"Created the temp_cred_file\")\n",
    "\n",
    "\n",
    "    # Get the credentials from the temporary file\n",
    "    scopes = [\"https://www.googleapis.com/auth/spreadsheets\"]\n",
    "    \n",
    "    creds = Credentials.from_service_account_file(temp_cred_file, scopes=scopes)\n",
    "\n",
    "    # Remove the temporary file\n",
    "    os.remove(temp_cred_file)\n",
    "\n",
    "    return creds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_worksheet_df_by_name(workbook,worksheet_name):\n",
    "\n",
    "    worksheet_list = map(lambda x: x.title, workbook.worksheets())\n",
    "    if worksheet_name not in worksheet_list:\n",
    "        logging.error(f\"Worksheet {worksheet_name} not found in the google sheet\")\n",
    "        raise ValueError(f\"Worksheet {worksheet_name} not found in the google sheet\")\n",
    "    # get the worksheet to df \n",
    "    return workbook.worksheet(worksheet_name)\n",
    "\n",
    "def a_ph(relative_path):\n",
    "    \"\"\"\n",
    "    Creates an absolute path based on the relative path from the project root.\n",
    "    \n",
    "    :param relative_path: Relative path from the project root\n",
    "    :return: Absolute path\n",
    "    \"\"\"\n",
    "    if current_app:\n",
    "        # If the function is called within the context of a Flask application\n",
    "        root_path = current_app.root_path\n",
    "    else:\n",
    "        # If the function is called outside the context of a Flask application\n",
    "        # Get the parent directory of the directory containing this file\n",
    "        root_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "    \n",
    "    # Join the root path with the relative path, removing any leading '/'\n",
    "    return os.path.join(root_path, relative_path.lstrip('/'))\n",
    "\n",
    "# Example usage:\n",
    "# print(a_ph('config/settings.json'))\n",
    "# print(a_ph('/static/images/logo.png'))\n",
    "\n",
    "def is_inv_updated_today():\n",
    "    # gen name of the download/update_files_07_18_24.zip \n",
    "    # extract the date from the name\n",
    "    # check if the date is today\n",
    "    # return True or False\n",
    "\n",
    "    zip_files = glob.glob(a_ph('/download/*.zip'))\n",
    "    if not zip_files:\n",
    "        return False\n",
    "    else:\n",
    "        zip_file = os.path.basename(zip_files[0])\n",
    "        date_str = zip_file.replace('update_files_', '').replace('.zip', '')\n",
    "        today = time.strftime('%m_%d_%y')\n",
    "        return date_str == today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Getting google sheets api credentials\n",
      "INFO:__main__:Created the temp_cred_file\n",
      "INFO:__main__:Updating google sheet \"<Worksheet 'walmart_sku_mapping' id:1392378980>\" in chunks\n",
      "INFO:__main__:Updating chunk A1:F2000\n",
      "INFO:__main__:Updating chunk A2001:F4000\n",
      "INFO:__main__:Updating chunk A4001:F6000\n",
      "INFO:__main__:Updating chunk A6001:F8000\n",
      "INFO:__main__:Updating chunk A8001:F10000\n",
      "INFO:__main__:Updating chunk A10001:F12000\n",
      "INFO:__main__:Updating chunk A12001:F14000\n",
      "INFO:__main__:Updating chunk A14001:F14988\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# update worksheet  with final_NA_mapping\n",
    "workbook = get_workbook()\n",
    "worksheet = get_worksheet_df_by_name(workbook, \"walmart_sku_mapping\")\n",
    "update_sheet_in_chunks(worksheet, final_NA_mapping, chunk_size=2000, continue_from_end=False, start_row=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inv_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
